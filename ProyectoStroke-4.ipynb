{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a614a5c-de60-4c3f-9a26-ceaee847daa0",
   "metadata": {},
   "source": [
    "## Título: \"Predicción de la ocurrencia de stroke basado en parámetros clínicos de fácil acceso\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16eeb3-6c4d-4dc4-bd94-aded97e1caa9",
   "metadata": {},
   "source": [
    "## Introducción y marco teórico\n",
    "\n",
    "El accidente cerebrovascular (ACV), también conocido como Stroke, es globalmente la segunda causa de muerte, dando cuenta del 11,6% del total de muertes en el 2019. Atribuyéndole a esta patología 63,48 millones de años de vida ajustado por discapacidad (AVISA), y 3,29 millones de muertes (1). En el 2020 la prevalencia global de todos los subtipos de ACV fue de 89,13 millones de casos, siendo el ACV de tipo isquémico el más frecuente, con un total de 68,16 millones de casos (2). \n",
    "Las predicciones globales muestran que la tasa de incidencia de ACV isquémico aumentarán en el tiempo, en ambos sexos, en todos los grupos etarios y quintiles socio-demográficos (3). \n",
    "Respecto a Chile, según el estudio de carga global de enfermedades, lesiones y factores de riesgo, el ACV representa el 9,1% del total de muertes en el 2017, y fue la segunda mayor causa de muerte general y muerte prematura, representando la tercera causa más común de muerte y discapacidad combinada en Chile durante ese año (4).\n",
    "En el ACV de tipo isquémico, es el subtipo más frecuente de ACV (1). El fenómeno corresponde a la oclusión de un vaso arterial que determina isquemia en el tejido por falta de irrigación sanguínea. Esto lleva a muerte del tejido cerebral (infarto cerebral), mostrando el paciente un déficit neurológico, el cual puede persistir, generando discapacidad en grados variables y/o la muerte. Es por eso que contar con formas de poder predecir su ocurrencia tienen una alta relevancia a nivel de salud pública global. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a61abe-8c2c-4447-be85-08519c88e7ea",
   "metadata": {},
   "source": [
    "## Objetivo \n",
    "El objetivo del presente proyecto es desarrollar un modelo de Machine Learning capaz de predecir la ocurrencia de un accidente cerebrovascular isquémico (Stroke) en pacientes, utilizando características clínicas simples y acotadas, basándonos en una base de datos pública.  \n",
    "\n",
    "## Base de datos\n",
    "\n",
    "Se trabajará con la base de datos pública extraída de Kaggle \"shashwatwork/cerebral-stroke-predictionimbalaced-dataset\" (5), que corresponde a un dataset que contiene distintas características demográficas y clínicas de pacientes, junto a la variable de interés que muestra la ocurrencia o no de un Accidente Cerebrovascular (ACV, también llamados Stroke).\n",
    "Cabe destacar que es una base de datos desbalanceada en relación con la variable de interés (Stroke), estando altamente cargada hacia casos sin ocurrencia de stroke. \n",
    "Esta base de datos tiene la ventaja de contar con variables y características de fácil acceso a la hora de evaluar inicialmente a un paciente, por lo que de resultar en un modelo de predicción robusto, tendría una buena utilidad clínica, permitiéndonos obtener beneficios para el paciente en base a parámetros de fácil obtención. \n",
    "\n",
    "Link a la base de datos: https://www.kaggle.com/datasets/shashwatwork/cerebral-stroke-predictionimbalaced-dataset/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc78dd-59ce-462b-bb7b-fb5d1c5096ec",
   "metadata": {},
   "source": [
    "## EXPLORACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43886f5f-055d-4f97-846f-9dc05d3709a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías para el análisis inicial de los datos. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "df = pd.read_csv('dataset.csv') #Cargo la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff528a0c-1d16-491a-9b36-dc0b97a4b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of           id  gender   age  hypertension  heart_disease ever_married  \\\n",
      "0      30669    Male   3.0             0              0           No   \n",
      "1      30468    Male  58.0             1              0          Yes   \n",
      "2      16523  Female   8.0             0              0           No   \n",
      "3      56543  Female  70.0             0              0          Yes   \n",
      "4      46136    Male  14.0             0              0           No   \n",
      "...      ...     ...   ...           ...            ...          ...   \n",
      "43395  56196  Female  10.0             0              0           No   \n",
      "43396   5450  Female  56.0             0              0          Yes   \n",
      "43397  28375  Female  82.0             1              0          Yes   \n",
      "43398  27973    Male  40.0             0              0          Yes   \n",
      "43399  36271  Female  82.0             0              0          Yes   \n",
      "\n",
      "          work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
      "0          children          Rural              95.12  18.0              NaN   \n",
      "1           Private          Urban              87.96  39.2     never smoked   \n",
      "2           Private          Urban             110.89  17.6              NaN   \n",
      "3           Private          Rural              69.04  35.9  formerly smoked   \n",
      "4      Never_worked          Rural             161.28  19.1              NaN   \n",
      "...             ...            ...                ...   ...              ...   \n",
      "43395      children          Urban              58.64  20.4     never smoked   \n",
      "43396      Govt_job          Urban             213.61  55.4  formerly smoked   \n",
      "43397       Private          Urban              91.94  28.9  formerly smoked   \n",
      "43398       Private          Urban              99.16  33.2     never smoked   \n",
      "43399       Private          Urban              79.48  20.6     never smoked   \n",
      "\n",
      "       stroke  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "43395       0  \n",
      "43396       0  \n",
      "43397       0  \n",
      "43398       0  \n",
      "43399       0  \n",
      "\n",
      "[43400 rows x 12 columns]>\n",
      "(43400, 12)\n",
      "520800\n"
     ]
    }
   ],
   "source": [
    "print(df.info)\n",
    "print(df.shape) \n",
    "print(df.size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bc816-a33e-40c9-a97b-72d83110f71f",
   "metadata": {},
   "source": [
    "## Cantidad de elementos\n",
    "El número total de elementos es de 520.800. \n",
    "El conjunto de datos contiene 43.400 filas y 12 columnas, utilizando la función df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "611f3639-b834-4658-81b8-a66d96843c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
      "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
      "       'smoking_status', 'stroke'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns) # se obtuvo los nombres de las 12 columnas contenidas en el data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73d4dc8f-d6e3-4d6f-8c63-e39e0afb83c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     int64\n",
      "gender                object\n",
      "age                  float64\n",
      "hypertension           int64\n",
      "heart_disease          int64\n",
      "ever_married          object\n",
      "work_type             object\n",
      "Residence_type        object\n",
      "avg_glucose_level    float64\n",
      "bmi                  float64\n",
      "smoking_status        object\n",
      "stroke                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes) #Se obtuvieron los tipos de datos de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c56ef63-6193-4098-a3e1-63807e00206b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43400.000000</td>\n",
       "      <td>43400.000000</td>\n",
       "      <td>43400.000000</td>\n",
       "      <td>43400.000000</td>\n",
       "      <td>43400.000000</td>\n",
       "      <td>41938.000000</td>\n",
       "      <td>43400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>36326.142350</td>\n",
       "      <td>42.217894</td>\n",
       "      <td>0.093571</td>\n",
       "      <td>0.047512</td>\n",
       "      <td>104.482750</td>\n",
       "      <td>28.605038</td>\n",
       "      <td>0.018041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21072.134879</td>\n",
       "      <td>22.519649</td>\n",
       "      <td>0.291235</td>\n",
       "      <td>0.212733</td>\n",
       "      <td>43.111751</td>\n",
       "      <td>7.770020</td>\n",
       "      <td>0.133103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18038.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.540000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>36351.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.580000</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54514.250000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.070000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72943.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>291.050000</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           age  hypertension  heart_disease  \\\n",
       "count  43400.000000  43400.000000  43400.000000   43400.000000   \n",
       "mean   36326.142350     42.217894      0.093571       0.047512   \n",
       "std    21072.134879     22.519649      0.291235       0.212733   \n",
       "min        1.000000      0.080000      0.000000       0.000000   \n",
       "25%    18038.500000     24.000000      0.000000       0.000000   \n",
       "50%    36351.500000     44.000000      0.000000       0.000000   \n",
       "75%    54514.250000     60.000000      0.000000       0.000000   \n",
       "max    72943.000000     82.000000      1.000000       1.000000   \n",
       "\n",
       "       avg_glucose_level           bmi        stroke  \n",
       "count       43400.000000  41938.000000  43400.000000  \n",
       "mean          104.482750     28.605038      0.018041  \n",
       "std            43.111751      7.770020      0.133103  \n",
       "min            55.000000     10.100000      0.000000  \n",
       "25%            77.540000     23.200000      0.000000  \n",
       "50%            91.580000     27.700000      0.000000  \n",
       "75%           112.070000     32.900000      0.000000  \n",
       "max           291.050000     97.600000      1.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() # Con la función df.describe obtenemos un resumen estadístico de las columnas del conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e38765-bf27-4692-9d4e-03f34f7242de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b902e49-7a9c-49a9-99e1-c95697891ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e9e5dbd-5483-41c5-9701-d06b453e17e4",
   "metadata": {},
   "source": [
    "## Evaluación de datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de7f39e-005e-4fd2-8ace-4e287cf3682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  gender    age  hypertension  heart_disease  ever_married  \\\n",
      "0      False   False  False         False          False         False   \n",
      "1      False   False  False         False          False         False   \n",
      "2      False   False  False         False          False         False   \n",
      "3      False   False  False         False          False         False   \n",
      "4      False   False  False         False          False         False   \n",
      "...      ...     ...    ...           ...            ...           ...   \n",
      "43395  False   False  False         False          False         False   \n",
      "43396  False   False  False         False          False         False   \n",
      "43397  False   False  False         False          False         False   \n",
      "43398  False   False  False         False          False         False   \n",
      "43399  False   False  False         False          False         False   \n",
      "\n",
      "       work_type  Residence_type  avg_glucose_level    bmi  smoking_status  \\\n",
      "0          False           False              False  False            True   \n",
      "1          False           False              False  False           False   \n",
      "2          False           False              False  False            True   \n",
      "3          False           False              False  False           False   \n",
      "4          False           False              False  False            True   \n",
      "...          ...             ...                ...    ...             ...   \n",
      "43395      False           False              False  False           False   \n",
      "43396      False           False              False  False           False   \n",
      "43397      False           False              False  False           False   \n",
      "43398      False           False              False  False           False   \n",
      "43399      False           False              False  False           False   \n",
      "\n",
      "       stroke  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3       False  \n",
      "4       False  \n",
      "...       ...  \n",
      "43395   False  \n",
      "43396   False  \n",
      "43397   False  \n",
      "43398   False  \n",
      "43399   False  \n",
      "\n",
      "[43400 rows x 12 columns]\n",
      "id                       0\n",
      "gender                   0\n",
      "age                      0\n",
      "hypertension             0\n",
      "heart_disease            0\n",
      "ever_married             0\n",
      "work_type                0\n",
      "Residence_type           0\n",
      "avg_glucose_level        0\n",
      "bmi                   1462\n",
      "smoking_status       13292\n",
      "stroke                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.isnull()) # se evaluaron valores faltantes\n",
    "print(df.isnull().sum()) #Se realizó un conteo de valores faltantes por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7171dba4-69f8-4a02-8b32-a38145cd8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        18.0\n",
      "1        39.2\n",
      "2        17.6\n",
      "3        35.9\n",
      "4        19.1\n",
      "         ... \n",
      "43395    20.4\n",
      "43396    55.4\n",
      "43397    28.9\n",
      "43398    33.2\n",
      "43399    20.6\n",
      "Name: bmi, Length: 43400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Dado que en las variables bmi (índice de masa corporal) y smoking_status (hábito tabáquico) hay datos faltantes, debo buscar la forma más adecuada de rellenarlos. \n",
    "df['bmi'].describe()\n",
    "# La función df.describe muestra que la variable bmi tiene outliers (mínimo de 10 y máximo de 97), por lo que se define imputar la media a los valores nulos y no el promedio. \n",
    "median_value = df['bmi'].median()\n",
    "df['bmi'] = df['bmi'].fillna(median_value) #Se rellenan datos faltantes de columna bmi con la mediana. \n",
    "\n",
    "print(df['bmi'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef1b04e8-de62-4800-84c9-fd3d179bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con la variable smoking_status, se definió rellenar los faltantes con una nueva etiqueta \n",
    "#Se llamará \"desconocido\", ya que no se puede asumir el hábito tabáquico de esos pacientes. \n",
    "\n",
    "df['smoking_status'] = df['smoking_status'].fillna('desconocido')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27bc9547-04d6-4e70-8e48-3a5886900a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   0\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Tras la imputación realizo una prueba de conteo de valores faltantes por columna para verificar que se ha resuelto.\n",
    "print(df.isnull().sum()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e080fcb-fe99-483d-91c2-021bbd065c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realizará transformación de las variables categóricas\n",
    "#Utilizando Codificación Label para asignar un número único a cada categoría de cada variable\n",
    "#Se hace este ejercicio con las variables de género, estado civil, tipo de trabajo, tipo de residencia y hábito tabáquico\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df['gender'] = encoder.fit_transform(df['gender'])\n",
    "df['ever_married'] = encoder.fit_transform(df['ever_married'])\n",
    "df['work_type'] = encoder.fit_transform(df['work_type'])\n",
    "df['Residence_type'] = encoder.fit_transform(df['Residence_type'])\n",
    "df['smoking_status'] = encoder.fit_transform(df['smoking_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3543447d-337f-4f25-ba62-72aeb0d557f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  gender   age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  30669       1   3.0             0              0             0          4   \n",
      "1  30468       1  58.0             1              0             1          2   \n",
      "2  16523       0   8.0             0              0             0          2   \n",
      "3  56543       0  70.0             0              0             1          2   \n",
      "4  46136       1  14.0             0              0             0          1   \n",
      "\n",
      "   Residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
      "0               0              95.12  18.0               0       0  \n",
      "1               1              87.96  39.2               2       0  \n",
      "2               1             110.89  17.6               0       0  \n",
      "3               0              69.04  35.9               1       0  \n",
      "4               0             161.28  19.1               0       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43400 entries, 0 to 43399\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 43400 non-null  int64  \n",
      " 1   gender             43400 non-null  int64  \n",
      " 2   age                43400 non-null  float64\n",
      " 3   hypertension       43400 non-null  int64  \n",
      " 4   heart_disease      43400 non-null  int64  \n",
      " 5   ever_married       43400 non-null  int64  \n",
      " 6   work_type          43400 non-null  int64  \n",
      " 7   Residence_type     43400 non-null  int64  \n",
      " 8   avg_glucose_level  43400 non-null  float64\n",
      " 9   bmi                43400 non-null  float64\n",
      " 10  smoking_status     43400 non-null  int64  \n",
      " 11  stroke             43400 non-null  int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 4.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se realiza chequeo de las variables tras el nueva codificación \n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c228c-71d7-489b-8fd9-1ac05dc75a7d",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "\n",
    "Se realiza normalización de variables continuas (Edad, índice de masa corporal y nivel de glucosa).\n",
    "- Para Variable Edad (Age) se realizará normalización con MinMaxScaler, pues necesito mantener las proporciones dentro de un rango fijo\n",
    "- Con variables de Índice de masa corporal (bmi) y niveles de glucosa (avg_glucose_level), utilizaré RobustScaler de tal forma de poder lidiar con los outliers, evitando que me distorcionen la escala.\n",
    "\n",
    "A continuación los códigos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "093dedca-a57d-4d1b-bdf1-2b44233c0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para variable edad\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_age = MinMaxScaler()\n",
    "df['age_scaled'] = scaler_age.fit_transform(df[['age']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b18bd166-2bed-4068-b504-3cd999222f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para variables bmi y avg_glucosa_level \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler_robust = RobustScaler()\n",
    "df['bmi_scaled'] = scaler_robust.fit_transform(df[['bmi']])\n",
    "df['glucose_scaled'] = scaler_robust.fit_transform(df[['avg_glucose_level']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29431b26-0ab2-44fd-a56d-55a8b7e97039",
   "metadata": {},
   "source": [
    "## Manejo del desbalanceo \n",
    "Ya teniendo una base datos explorada y trabajada, pasaremos al desarrollo del modelo, sin embargo, tal como indica el nombre del dataset, la base de datos está desbalanceada, por lo que se debe balancear los datos en relación a la variable de interés 'stroke'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04e39287-9f04-4e8d-8f3b-021c7b71a4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stroke\n",
       "0    42617\n",
       "1      783\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se aplica función para evidenciar cuántos casos presentaron (1) o No presentaron (0) un stroke y dar cuenta del desbalanceo.\n",
    "df['stroke'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053445d-6e1a-407b-891c-cef06b8bdb78",
   "metadata": {},
   "source": [
    "\n",
    "Los datos están desbalanceados porque la ocurrencia del ACV está en 783 pacientes, versus la no ocurrencia que está en 42.617 (relación 1:54). Por lo que hacer predicciones utilizando estos datos en estas condiciones no resultaría confiable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a675dc8-4c77-42fd-99e3-0003c245c908",
   "metadata": {},
   "source": [
    "## Eligiendo el método de balanceo y entrenamiento del modelo\n",
    "\n",
    "Dado que los datos están desbalanceados, se probarán técnicas de balanceo y al mismo tiempo se realizará el entrenamiento del modelo con Random Forest y evaluación de métricas (Accuracy, Precision, Recall, F1-Score y Matriz de confusión).\n",
    "\n",
    "MÉTRICAS SELECCIONADAS: \n",
    "- Precision: Permitirá determinar cuántos casos en que se predice stroke, realmente lo tendrán. \n",
    "- Recall: Nos permite evaluar si de todos los pacientes que realmente tienen stroke, cuántos son detectados. Es una métrica importantísima porque nos indica la capacidad que tiene el modelo para detectar la mayor cantidad posible de casos reales, pues no queremos dejar casos reales sin detectar. \n",
    "- F1-Score: Realiza un balance entre precision y Recall. Es un buen resumen numérico del desempeño global de nuestro modelo. \n",
    "- Matriz de Confusión: Es relevante porque nos mostrará cuántos stroke detecta y cuántos falsos positivos genera.\n",
    "\n",
    "¿Por qué Random Forest?\n",
    "- El Random Forest es un algoritmo que se basa en el ensamblaje de árboles de decisión, combinando varios árboles de forma independiente, y posteriormente realiza una promediación de las predicciones.\n",
    "- Es una buena técnica para reducir el sobreajuste, detecta interacciones entre variables que podrían no ser tran evidentes y es robusto frente al ruido. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea4c8968-19f4-4ed0-a00a-dcec280b5bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución original en train: Counter({0: 29832, 1: 548})\n"
     ]
    }
   ],
   "source": [
    "# Se importan las librerías necesarias. \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "#Defino las variables\n",
    "X = df.drop(['id', 'stroke'], axis=1)\n",
    "y = df['stroke']\n",
    "\n",
    "#Se realiza el split de entrenamiento y test antes de aplicar técnicas de balanceo. \n",
    "# se define una partición de entrenamiento/prueba de 70/30. \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Distribución original en train:\", Counter(y_train))\n",
    "\n",
    "#Preprocesamiento, defino explícitamente las categorías.  \n",
    "\n",
    "categorical_categories = {\n",
    "    'gender': ['Male', 'Female', 'Other'],\n",
    "    'ever_married': ['No', 'Yes'],\n",
    "    'work_type': ['children', 'Govt_job', 'Never_worked', 'Private', 'Self-employed'],\n",
    "    'Residence_type': ['Rural', 'Urban'],\n",
    "    'smoking_status': ['formerly smoked', 'never smoked', 'smokes', 'Unknown']\n",
    "}\n",
    "\n",
    "#Lista de columnas categóricas y numéricas\n",
    "categorical_cols = list(categorical_categories.keys())\n",
    "numeric_cols = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#FUNCIONES DE EVALUACIÓN\n",
    "def evaluate_model(X_train_res, y_train_res, X_test, y_test, title):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train_res, y_train_res)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n=== Resultados: {title} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97e736-2edc-4209-be62-5468ff7c5ac1",
   "metadata": {},
   "source": [
    "Puntos relevantes en el código: \n",
    "\n",
    "- Se utiliza test size de 0,3 (30%): de esta forma el test set tendrá aproximadamente 13.000 registros, permitiendo evaluar el modelo con un número suficiente de casos strokes y obtener métricas confiables.\n",
    "- Además se utiliza stratify=y, asegurándonos que mantendremos esa misma proporción en ambas clases tanto en el conjunto de entrenamiento y de prueba. Esto es aún más importantes en dataset tan desbalanceados como este.\n",
    "- El random state tiene como fin suprimir la aleatoriedad. \n",
    "\n",
    "- Resulta importante aclarar que siempre se realizará primero la división train/test (en el dataset desbalanceado) y luego se aplicará la técnica de balanceo, de esta forma el método de balanceo se aplicará solamente en el conjunto de entrenamiento. Así el modelo se entrena con datos balanceados, pero se testeará con el dataset original que no debe ser manipulado. Así nos aseguramos que la evaluación represente casos reales.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7b1e7-f563-45db-b8e5-e39261be1154",
   "metadata": {},
   "source": [
    "Se probarán 3 técnicas de balanceo: Random oversampling, SMOTE y SMOTEENN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975896d-245a-4698-9bd8-fdf622a11766",
   "metadata": {},
   "source": [
    "## RANDOM OVERSAMPLING\n",
    "\n",
    "Técnica de balanceo que toma los casos minoritarios (en este caso, pacientes con Stroke) y los copia hasta que haya la misma cantidad que la clase mayoritaria. \n",
    "Tiene la ventaja de ser más fácil de aplicar, manteniendo además la información original. \n",
    "Por otro lado, tiene la desventaja de presentar alto riesgo de overfitting, porque el modelo usa los mismos ejemplos repetidamente, y no agrega diversidad a la clase minoritaria de interés. \n",
    "\n",
    "Vamos a evaluar la aplicación de este método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0755632e-627c-41bb-a5a6-df33bc5d4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución después de Random Oversampling: Counter({0: 29832, 1: 29832})\n",
      "\n",
      "=== Resultados: Random Oversampling ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9822    0.9980    0.9901     12785\n",
      "           1     0.1379    0.0170    0.0303       235\n",
      "\n",
      "    accuracy                         0.9803     13020\n",
      "   macro avg     0.5601    0.5075    0.5102     13020\n",
      "weighted avg     0.9670    0.9803    0.9727     13020\n",
      "\n",
      "Matriz de confusión:\n",
      " [[12760    25]\n",
      " [  231     4]]\n"
     ]
    }
   ],
   "source": [
    "#RANDOM OVERSAMPLING\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "print(\"Distribución después de Random Oversampling:\", Counter(y_ros))\n",
    "\n",
    "evaluate_model(X_ros, y_ros, X_test, y_test, \"Random Oversampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f628c-a645-4707-9835-1a4d9096bbf2",
   "metadata": {},
   "source": [
    "Con Random Oversampling se logró balancear numéricamente el dataset (29.832 casos en cada clase). \n",
    "Yendo a las métricas: \n",
    "- En la clase 0 (pacientes sin Stroke) hay una precisión del 98,2%, por lo que suele acertar casi siempre en estos casos, con un recall de 99,8%, por lo que detecta prácticamente todos los pacientes Sin Stroke. Sin embargo, esta no es la clase que nos importa.\n",
    "- En la clase 1 (Stroke), predice stroke sólo en el 13,7% de los casos, por lo que hay muchos falsos positivos. El Recall es de 1,7%, es decir detecta sólo el 1,7% de los pacientes que sí tienen stroke.\n",
    "- El F1-Score es 0,03: extremadamente bajo. Confirmando que de esta forma el modelo no es capaz de predecir riesgo de strokes. \n",
    "\n",
    "MATRIZ DE CONFUSIÓN \n",
    "- Verdaderos positivos: 4 (sólo detectó 4 casos reales de stroke)\n",
    "- Falsos negativos: 231 (231 casos de stroke no fueron identificados, lo cual es grave)\n",
    "- Falsos positivos: 25.\n",
    "\n",
    "MÉTRICAS GLOBALES \n",
    "- Accuracy de 98%, lo cual es alto, pero no relevante en el presente trabajo, ya que el modelo sólo predice adecuadamente a la clase mayoritaria.\n",
    "\n",
    "En suma, modelo no útil para los objetivos del dataset que es detectar strokes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970db8bc-31da-4074-af25-a3f7b620647f",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "Es un método que genera nuevos casos de forma sintética para la clase minoritaria (Stroke), combinando características de los pacientes reales y cercanos entre sí. De esta manera balancea ambas clases de una forma más variada que con Random Oversampling y hay menos riesgo de overfitting. \n",
    "El problema es que pudiesen crearse casos poco realistas si es que existiesen datos muy atípicos o no correctamente etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a110d8d5-c671-491a-9bea-1a7ddd2f75d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución después de SMOTE: Counter({0: 29832, 1: 29832})\n",
      "\n",
      "=== Resultados: SMOTE ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9832    0.9682    0.9756     12785\n",
      "           1     0.0557    0.1021    0.0721       235\n",
      "\n",
      "    accuracy                         0.9525     13020\n",
      "   macro avg     0.5195    0.5351    0.5239     13020\n",
      "weighted avg     0.9665    0.9525    0.9593     13020\n",
      "\n",
      "Matriz de confusión:\n",
      " [[12378   407]\n",
      " [  211    24]]\n"
     ]
    }
   ],
   "source": [
    "#SMOTE \n",
    "#Ajustar k_neighbors de forma dinámica para evitar errores. \n",
    "minority_count = Counter(y_train)[1]\n",
    "k_neighbors = min(5, minority_count - 1)\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"Distribución después de SMOTE:\", Counter(y_smote))\n",
    "\n",
    "evaluate_model(X_smote, y_smote, X_test, y_test, \"SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece36e77-b287-47d7-a8ca-9632d1ff21ad",
   "metadata": {},
   "source": [
    "Con SMOTE se balanceó el dataset, con 29.832 casos en cada grupo, con la vetaja de que en estos casos no son duplicados, sino que son generados de manera sintética, a diferencia del random oversampling. \n",
    "- La clase 0 (No stroke) obtiene una precisión de 98,3% y recall de 96,2%. Por lo que tiene buen desempeño en identificar pacientes sin stroke.\n",
    "- En la clase 1 (Stroke) muestra una precisión de 5,57%, es decir, de los casos en donde identifica stroke, sólo el 5,57% de las veces son casos reales, por lo que hay muchos falsos positivos. Con Recall de 10,2%, por lo que estaría detectando sólo 1 de cada 10 casos reales de stroke.\n",
    "- F1-score de 0,07, lo que es muy bajo, evidenciando que no es capaz de aprender sólidamente los patrones para la clase 1.\n",
    "\n",
    "MATRIZ DE CONFUSIÓN \n",
    "- Verdaderos positivos: 24 (sólo detectó 24 casos reales de stroke)\n",
    "- Falsos negativos: 211 (211 pacientes con stroke No fueron identificados)\n",
    "- Falsos positivos: 407 (muy alto para un modelo de predicción clínica). \n",
    "\n",
    "ACCURACY: 95%. Buen accuracy, pero al igual que en Random Oversampling, no relevante para el objetivo del trabajo, ya que es un modelo que sólo detecta bien el No Stroke. \n",
    "\n",
    "En suma, modelo no adecuado para el objetivo de detectar stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51309641-46bf-4d51-8400-4abb321f1eb3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## UNDERSAMPLING \n",
    "\n",
    "Se desestima usar undersampling porque se reduciría enormemente la clase mayoritaria, perdería mucha  médica relevante y disminuiría la capacidad del modelo para poder aprender patrones, hay más riesgo de terminar con un modelo demasiado simple incapaz de generalizar, además de aumentar la varianza, llevando a que pequeños cambios en la selección de datos produzcan gran variación de los resultados, haciendo el modelo inestable y dificil de reproducir. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68579d84-8833-4101-bce0-11bf49e15463",
   "metadata": {},
   "source": [
    "## SMOTEENN \n",
    "\n",
    "Es una técnica que permite combinar oversampling con undersampling de manera más inteligente. Es híbrida y consta principalmente de 2 pasos: \n",
    "\n",
    "1.- SMOTE, en donde genera muestras sintéticas de la clase minoritaria (Stroke = 1), de esta forma se aumenta la representación de este grupo. Además ayuda a mi modelo a aprender patrones más generales, sin memorizar ejemplos. \n",
    "2.- ENN: Tras el SMOTE, se eliminan los ejemplos de ambas clases que se encuentren mal clasificados por sus vecinos más cercanos, de esta forma me permite eliminar ruido. \n",
    "\n",
    "Por lo que sería una técnica conveniente de aplicar en este dataset muy desbalanceado. \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f56683f4-5db1-428e-b06d-ad7b16c4d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución después de SMOTEENN: Counter({1: 29382, 0: 25062})\n",
      "\n",
      "=== Resultados: SMOTEENN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9851    0.9387    0.9613     12785\n",
      "           1     0.0644    0.2298    0.1007       235\n",
      "\n",
      "    accuracy                         0.9259     13020\n",
      "   macro avg     0.5248    0.5842    0.5310     13020\n",
      "weighted avg     0.9685    0.9259    0.9458     13020\n",
      "\n",
      "Matriz de confusión:\n",
      " [[12001   784]\n",
      " [  181    54]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Ajustar k_neighbors para SMOTE dentro de SMOTEENN\n",
    "minority_count = Counter(y_train)[1]\n",
    "k_neighbors = min(5, minority_count - 1)\n",
    "\n",
    "smote_enn = SMOTEENN(\n",
    "    smote=SMOTE(random_state=42, k_neighbors=k_neighbors),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_smoteenn, y_smoteenn = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Distribución después de SMOTEENN:\", Counter(y_smoteenn))\n",
    "\n",
    "evaluate_model(X_smoteenn, y_smoteenn, X_test, y_test, \"SMOTEENN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c98d72-36e6-464d-aa75-a19eb7bdbc74",
   "metadata": {},
   "source": [
    "Con SMOTEENN se balanceó adecuadamente casi 50/50 (Clase 1: 29.382 casos, y clase 0: 25.062 casos), ya que generó casos sintéticos de la clase minoritaria vía SMOTE. Además eliminó el ruido y ejemplos que pudiesen generar conflicto, dando finalmente un dataset más balanceado. \n",
    "\n",
    "- La clase 0 (No stroke) mostró una precision de 98,5%, similar a los métodos previamente evaluados, dando cuenta que casi siempre acierta si se trata de predecir No Stroke. Con un Recall de 93,9%, detectando correctamente pacientes sanos. Buenas cifras para la variable mayoritaria, sin embargo, no es el objetivo perseguido por este trabajo.\n",
    "- Para la Clase 1 (Stroke) arroja una precision de 6,4%, es decir, cuando predice que hay stroke, sólo el 6,4% de las veces es real. Esto indica una alta tasa de falsos positivos. Respecto al Recall se obtuvo un 22,9%, detectando correctamente sólo ese porcentaje de pacientes que realmente tienen stroke. Eso significa que estamos perdiendo una gran mayoría de casos de stroke. Y un F1-Score de 0,1, lo cual confirma que el modelo tiene problemas en la detección de la clase de interés.\n",
    "\n",
    "MATRIZ DE CONFUSIÓN \n",
    "- Falsos negativos: 181 pacientes con stroke no fueron detectados.\n",
    "- Verdaderos positivos: 54 casos reales de stroke fueron detectados.\n",
    "- En la misma línea, confirma que el modelo no estaría siendo capaz de capturar correctamente la clase 1.\n",
    "\n",
    "En suma, si bien son cifras mejores que en los entrenamientos con los balanceos previamente descritos, sigue siendo un modelo inconsistente en cuanto a la detección de strokes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54a9c6-2433-4034-905a-bd4dea8ca8d7",
   "metadata": {},
   "source": [
    "## Evaluación del sobreajuste\n",
    "Ya que se definió que el balanceo con el método de SMOTEENN obtuvo las mejores métricas, es momento de evaluar el sobreajuste del modelo. \n",
    "\n",
    "Lo que se hará será comparar el rendimiento en el set de entrenamiento (que determina cómo se comporta con los datos con los que fue entrenado) con el set de prueba (para evaluar qué tan bien generaliza el modelo). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64f885d0-c564-416a-be4c-74282c7b0515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluación del modelo: SMOTEENN ===\n",
      "\n",
      "--- Entrenamiento ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     25062\n",
      "           1     1.0000    1.0000    1.0000     29382\n",
      "\n",
      "    accuracy                         1.0000     54444\n",
      "   macro avg     1.0000    1.0000    1.0000     54444\n",
      "weighted avg     1.0000    1.0000    1.0000     54444\n",
      "\n",
      "\n",
      "--- Prueba ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9851    0.9387    0.9613     12785\n",
      "           1     0.0644    0.2298    0.1007       235\n",
      "\n",
      "    accuracy                         0.9259     13020\n",
      "   macro avg     0.5248    0.5842    0.5310     13020\n",
      "weighted avg     0.9685    0.9259    0.9458     13020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_overfitting(X_train_res, y_train_res, X_test, y_test, title):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Predicciones en entrenamiento\n",
    "    y_pred_train = pipeline.predict(X_train_res)\n",
    "    # Predicciones en test (prueba). \n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n=== Evaluación del modelo: {title} ===\")\n",
    "    print(\"\\n--- Entrenamiento ---\")\n",
    "    print(classification_report(y_train_res, y_pred_train, digits=4))\n",
    "    \n",
    "    print(\"\\n--- Prueba ---\")\n",
    "    print(classification_report(y_test, y_pred_test, digits=4))\n",
    "    \n",
    "    return pipeline\n",
    "    \n",
    "model = evaluate_overfitting(X_smoteenn, y_smoteenn, X_test, y_test, \"SMOTEENN\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed53ef0-8ece-40ce-9590-566ee3b27550",
   "metadata": {},
   "source": [
    "## ¿Es un modelo sobreajustado?\n",
    "\n",
    "El sobreajuste ocurre cuando un modelo aprende excesivamente bien los patrones y el ruido del conjunto de entrenamiento. Así el modelo pierde su capacidad de generalizar. \n",
    "\n",
    "En este caso las métricas muestran que el modelo está severamente sobreajustado, mostrando en el set de prueba todas las métricas en 1.0, dando cuenta de la memorización, con un set de prueba que da muy pobres parámetros en la clase 1 (precision 0.064, recall 0.230, F1 0.101). \n",
    "El accuracy de 0,92 en este caso puede ser engañoso dado el alto desbalance mostrado. \n",
    "En suma, es un modelo incapaz de generalizar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f6959-075e-474a-ae03-62bbbbccab7e",
   "metadata": {},
   "source": [
    "## Interpretación general de resultados\n",
    "\n",
    "En medicina, en especial en la predicción de enfermedades graves como el stroke, el recall de la clase positiva es sumamente importante. En este caso, el modelo mostró un recall de 22,9%. En otras palabras, el modelo está perdiendo la detección de 3 de cada 4 pacientes que son positivos para stroke, con un F1-score de 0,1 (muy bajo) por lo que el desempeño de la clase positiva es extremadamente pobre. Del punto de vista clínico es insostenible porque estas cifras dan cuenta que el modelo no está siendo capaz de detectar los casos. \n",
    "Además, el alto sobreajuste evidenciado nos habla de un modelo que no aprende a clasificar la clase 1 y lo ineficiente que es para generalizar.  \n",
    "El sobreajuste elevado puede pasar por el extremo desbalanceo del dataset, incluso con técnicas de balanceo como SMOTEENN, la señal de clase 1 (stroke) es muy débil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83c5e5-8881-41a5-b510-60ebe81a26b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1e895b-0967-45a9-8019-5610ea6455cd",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "Es un modelo poco útil para toma de decisiones clínicas por su baja capacidad de predicción, no cumpliéndose el objetivo principal del dataset.  \n",
    "Las razones podrían pasar justamente por la simpleza de las variables utilizadas, dado que la ocurrencia de un ACV es tremendamente multifactorial e intentar definirlo sólo a través de las 12 variables contenidas en este dataset puede ser algo reduccionista. Afuera quedan antecedentes cruciales de los pacientes y que ya se sabe que confieren un alto riesgo de stroke como ciertos tipos de arritmias cardíacas, la presencia de cáncer, trastornos del colesterol, diagnóstico de diabetes, la adherencia al tratamiento farmacológico de sus enfermedades crónicas, entre muchas otras. Todos ellos son elementos que están fuera de las características contenidas en este dataset. Este motivo, además del alto desbalanceo del dataset, son probablemente las razones más sigificativas de por qué los resultados no son consistentes, pese a las diferentes técnicas de balanceo probadas. \n",
    "\n",
    "Se hace necesario el desarollo de futuros modelos que predigan la ocurrencia de stroke en pacientes, sustentados en bases de datos que involucren una mayor cantidad de variables clínicas relevantes asociadas al accidente cerebrovascular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d1351-4dc3-48c6-bc98-50e9f8bd49c7",
   "metadata": {},
   "source": [
    "## CITACIÓN Y REFERENCIAS \n",
    "1.\tFeigin VL, Stark BA, Johnson CO, Roth GA, Bisignano C, Abady GG, Abbasifard M, Abbasi-Kangevari M, Abd-Allah F, Abedi V, et al. Global, regional, and national burden of stroke and its risk factors, 1990-2019: a systematic analysis for the global burden of disease study 2019.Lancet Neurol. 2021; 20:795–820.\n",
    "2.\tCapirossi C, Laiso A, Renieri L, Capasso F, Limbucci N. Epidemiology, organization, diagnosis and treatment of acute ischemic stroke. Eur J Radiol Open [Internet]. 2023;11(100527):100527. Disponible en: http://dx.doi.org/10.1016/j.ejro.2023.100527\n",
    "3.\tPu L, Wang L, Zhang R, Zhao T, Jiang Y, Han L. Projected global trends in ischemic stroke incidence, deaths and disability-adjusted life years from 2020 to 2030. Stroke [Internet]. 2023;54(5):1330–9. Disponible en: http://dx.doi.org/10.1161/strokeaha.122.040073\n",
    "4.\tKyu HH, Abate D, Abate KH, Abay SM, Abbafati C, Abbasi N, et al. Global, regional, and national disability-adjusted life-years (DALYs) for 359 diseases and injuries and healthy life expectancy (HALE) for 195 countries and territories, 1990–2017: a systematic analysis for the Global Burden of Disease Study 2017. Lancet [Internet]. 2018;392(10159):1859–922. Disponible en: http://dx.doi.org/10.1016/s0140-6736(18)32335-3\n",
    "5. Liu, Tianyu; Fan, Wenhui; Wu, Cheng (2019), “Data for: A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical-datasets”, Mendeley Data, V1, doi: 10.17632/x8ygrw87jw.1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8c8f4-964a-407c-9702-f674e41edf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
